
import requests
import click
import os
from os.path import join, getsize

#HOST = '127.0.0.1' '[0:0:0:0:0:0:0:1]' 'ip6-localhost'  'tenris'
HOST = 'localhost'

SOLR = 'http://' + HOST + ':8983/solr/blogsand/'

@click.command()
@click.option('--delete', default="none", help='delete all')
@click.option('--dirname', default="none",
              help='The files or dir to index.')
def primeOps( delete, dirname):
    if delete != "none":
        deleteDocs( delete)
        return
    # the dirname param is required for the indexing func
    if dirname != "none":
        indexfiles( dirname)
        return
    return

def deleteDocs( delete):
    """ clear the index in preparation for indexing."""
    if delete == "all":

        #deljson = '{ "delete": { "query": "*:*" },    "commit": {}  }'
        url = SOLR + 'update?commit=true'
        payload = {'delete': { "query": "*:*" }}

        r = requests.post(url, json=payload)
        r.raise_for_status()
        try:
             resjson = r.json()
             print( resjson)
        except ValueError:
            print( "No JSON object could be decoded")

def indexfiles( dirname): 
    """ indexes all files in a dir tree to Solr."""
    # the parameters passed to Solr for indexing
    url = SOLR + 'update/extract'
    url = url + '?uprefix=attr_'
    #url = url + '&commit=true'                         # use autoCommit instead
    url = url + '&extractOnly=false'
    url = url + '&fmap.og_url=id'                 # the id field should contain  the url
    url = url + '&literal.indexed_sitename=' + dirname  # dirname is used for a literal field

    #url = SOLR + "update/extract?uprefix=attr_&commit=true&extractOnly=true&capture=div&fmap.div=foooooot&xpath=/xhtml:html/xhtml:body//xhtml:div[@class='post-content']/node()"

    for root, dirs, files in os.walk(dirname):
        print(root, "consumes", end=" ")
        print(sum(getsize(join(root, name)) for name in files), end=" ")
        #        for name in files:
        #           print (getsize(join(root, name)) )
        print("bytes in", len(files), "non-directory files")
        #        print ("bytes in " + len(files) + " non-directory files")
        if 'CVS' in dirs:
            dirs.remove('CVS')  # don't visit CVS directories

        for filename in files:
            # postfiles = {'file': open(FILE, 'rb')}
            # postfiles = {'file': ('lw9.html', open(join(root,filename), 'rb'), 'text/html', {'Expires': '0'})}
            
            postfiles = {'file': (filename, open(join(root,filename), 'rb'), 'text/html', {'Expires': '0'})} # filename is indexed as the "attr_stream_name" field

            # how to get just the stuff we want from tika
            # the content

            # how to spider just for 2013 2012 etc subdirs (2009 is oldest) (could get there from blog)
            #start from current year

            #fmap.source_field
            #Maps (moves) one field name to another. The source_field must be a field in incoming documents, and the value is the Solr field to map to. Example: fmap.content=text causes the data in the content field generated by Tika to be moved to the Solrâ€™s text field.
            #"literal.id=doc2&captureAttr=true&defaultField=_text_&fmap.div=foo_t&capture=div"


            # ignoreTikaException
            r = requests.post(url, files=postfiles)
            r.raise_for_status()

            # data = {'key':'value'})
            try:
                resjson = r.json()
                print( resjson)
            except ValueError:
                print( "No JSON object could be decoded")
    # commit once (in case autocommit is not adequate)
    # the parameters passed to Solr for commit
    # if commit:
    #    url = SOLR + 'update'

if __name__ == '__main__':
    primeOps()
