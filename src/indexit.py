
import requests
import click
import os
from os.path import join, getsize

#HOST = '127.0.0.1' '[0:0:0:0:0:0:0:1]' 'ip6-localhost'  'tenris'
HOST = 'localhost'

SOLR = 'http://' + HOST + ':8983/solr/blogsand/'

@click.command()
@click.option('--delete', default="none", help='delete all')
@click.option('--dirname', default="none",
              help='The files or dir to index.')
def indexfiles( delete, dirname):
    """Simple program that indexes all files in a dir tree to Solr. It can also clear the index before indexing."""
    if delete == "all":

        #deljson = '{ "delete": { "query": "*:*" },    "commit": {}  }'
        url = SOLR + 'update?commit=true'
        payload = {'delete': { "query": "*:*" }}

        r = requests.post(url, json=payload)
        r.raise_for_status()
        try:
             resjson = r.json()
             print( resjson)
        except ValueError:
            print( "No JSON object could be decoded")
        
    if dirname == "none":
        return

    for root, dirs, files in os.walk(dirname):
        print(root, "consumes", end=" ")
        print(sum(getsize(join(root, name)) for name in files), end=" ")
        #        for name in files:
        #           print (getsize(join(root, name)) )
        print("bytes in", len(files), "non-directory files")
        #        print ("bytes in " + len(files) + " non-directory files")
        if 'CVS' in dirs:
            dirs.remove('CVS')  # don't visit CVS directories

        for filename in files:
            # postfiles = {'file': open(FILE, 'rb')}
            postfiles = {'file': ('lw9.html', open(join(root,filename), 'rb'), 'text/html', {'Expires': '0'})}

            # how to get just the stuff we want from tika
            # the content

            # an id (should be  the url)

            # how to spider just for 2013 2012 etc subdirs (2009 is oldest) (could get there from blog)
            #start from current year

            #fmap.source_field
            #Maps (moves) one field name to another. The source_field must be a field in incoming documents, and the value is the Solr field to map to. Example: fmap.content=text causes the data in the content field generated by Tika to be moved to the Solrâ€™s text field.
            #"literal.id=doc2&captureAttr=true&defaultField=_text_&fmap.div=foo_t&capture=div"


            
            url = SOLR + 'update/extract?uprefix=attr_&commit=true&extractOnly=false'
            #url = SOLR + 'update/extract?uprefix=attr_&commit=true&extractOnly=false'

            #url = SOLR + 'update/extract?uprefix=attr_&commit=true&extractOnly=true&capture=div&xpath=/xhtml:html/xhtml:body/xhtml:div[@class="post-content"]/node()'
            #url = SOLR + "update/extract?uprefix=attr_&commit=true&extractOnly=false&capture=div&fmap.div=foooooot&xpath=/xhtml:html/xhtml:body//xhtml:div[@class='post-content']/node()"

            # ignoreTikaException
            r = requests.post(url, files=postfiles)
            r.raise_for_status()

            # data = {'key':'value'})
            try:
                resjson = r.json()
                print( resjson)
            except ValueError:
                print( "No JSON object could be decoded")

if __name__ == '__main__':
    indexfiles()
